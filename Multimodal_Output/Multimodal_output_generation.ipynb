{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL+aDJ9RrDjsqA+32HwDJ5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HJ139Mkpw-ih"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from transformers import AutoTokenizer, AutoModel\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from typing import List, Dict, Tuple\n","import numpy as np\n","from sklearn.metrics import f1_score\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","import os\n","import random\n","from torch.optim.lr_scheduler import StepLR\n","\n","# Set random seeds\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","set_seed(42)\n","\n","class MultiFrameScorer(nn.Module):\n","    def __init__(self, pretrained: bool = True):\n","        super().__init__()\n","\n","        # Image encoder (ResNet-50); When frames extracted from gmflow is passed\n","        self.image_encoder = models.resnet50(pretrained=pretrained)\n","        self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1])\n","\n","        # Text encoder (BERT); Where generated summaries from Modified_DPO is passed\n","        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","        self.text_encoder = AutoModel.from_pretrained('bert-base-uncased')\n","\n","        # Projection layers\n","        self.image_projection = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.2)\n","        )\n","        self.text_projection = nn.Sequential(\n","            nn.Linear(768, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.2)\n","        )\n","\n","        # Frame sequence encoder (Transformer)\n","        self.frame_sequence_encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(\n","                d_model=512,\n","                nhead=8,\n","                dim_feedforward=2048,\n","                dropout=0.2\n","            ),\n","            num_layers=4\n","        )\n","\n","        # Frame selection attention\n","        self.frame_attention = nn.MultiheadAttention(\n","            embed_dim=512,\n","            num_heads=8,\n","            dropout=0.2\n","        )\n","\n","        # Image preprocessing\n","        self.image_transform = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.RandomHorizontalFlip(),  # Data augmentation\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                               std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def encode_frames(self, frames: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Encode batch of frames\"\"\"\n","        batch_size, num_frames = frames.shape[:2]\n","        # Reshape to encode all frames\n","        frames_flat = frames.view(-1, 3, 224, 224)\n","        features = self.image_encoder(frames_flat)\n","        features = features.squeeze()\n","        features = self.image_projection(features)\n","        # Reshape back to batch format\n","        return features.view(batch_size, num_frames, -1)\n","\n","    def encode_summary(self, summaries: List[str]) -> torch.Tensor:\n","        \"\"\"Encode text summaries\"\"\"\n","        tokens = self.tokenizer(summaries, padding=True, truncation=True,\n","                              return_tensors=\"pt\", max_length=128)\n","        tokens = {k: v.to(next(self.parameters()).device) for k, v in tokens.items()}\n","        text_features = self.text_encoder(**tokens).last_hidden_state.mean(dim=1)\n","        return self.text_projection(text_features)\n","\n","    def compute_similarity(self, frame_features: torch.Tensor, summary_features: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Compute cosine similarity between frame features and summary features.\n","        frame_features: [batch_size, num_frames, 512]\n","        summary_features: [batch_size, 512]\n","        Returns: [batch_size, num_frames] similarity scores\n","        \"\"\"\n","        # Normalize features\n","        frame_features = F.normalize(frame_features, p=2, dim=-1)\n","        summary_features = F.normalize(summary_features, p=2, dim=-1)\n","\n","        # Expand summary features to match frame features\n","        summary_features = summary_features.unsqueeze(1)  # [batch_size, 1, 512]\n","\n","        # Compute cosine similarity\n","        similarity = torch.sum(frame_features * summary_features, dim=-1)  # [batch_size, num_frames]\n","        return similarity\n","\n","    def forward(self, frames: torch.Tensor, summaries: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass returning similarity scores and attention weights.\n","        frames: [batch_size, num_frames, 3, 224, 224]\n","        \"\"\"\n","        # Encode frames and summary\n","        frame_features = self.encode_frames(frames)  # [batch_size, num_frames, 512]\n","        summary_features = self.encode_summary(summaries)  # [batch_size, 512]\n","\n","        # Compute similarity scores\n","        similarity_scores = self.compute_similarity(frame_features, summary_features)  # [batch_size, num_frames]\n","\n","        # Process frame sequence (optional, for attention weights)\n","        frame_seq = self.frame_sequence_encoder(frame_features.transpose(0, 1)).transpose(0, 1)\n","        summary_expanded = summary_features.unsqueeze(1)  # [batch_size, 1, 512]\n","        _, attention_weights = self.frame_attention(\n","            summary_expanded.transpose(0, 1),\n","            frame_seq.transpose(0, 1),\n","            frame_seq.transpose(0, 1)\n","        )\n","\n","        return similarity_scores, attention_weights\n","\n","\n","class MultiFrameDataset(Dataset):\n","    def __init__(self, frame_sequences: List[List[str]],\n","                 summaries: List[str],\n","                 target_frames: List[List[int]],\n","                 transform=None,\n","                 max_frames: int = 10):  # Maximum number of frames per sequence\n","        \"\"\"\n","        frame_sequences: List of lists containing paths to frame sequences\n","        summaries: List of summary texts\n","        target_frames: List of lists containing indices of correct frames\n","        max_frames: Maximum number of frames to pad/truncate to\n","        \"\"\"\n","        self.frame_sequences = frame_sequences\n","        self.summaries = summaries\n","        self.target_frames = target_frames\n","        self.transform = transform\n","        self.max_frames = max_frames\n","\n","    def __len__(self):\n","        return len(self.summaries)\n","\n","    def __getitem__(self, idx):\n","        # Load all frames for this sequence\n","        frames = []\n","        for frame_path in self.frame_sequences[idx]:\n","            image = Image.open(frame_path).convert('RGB')\n","            if self.transform:\n","                image = self.transform(image)\n","            frames.append(image)\n","\n","        # Pad or truncate frames to a fixed length\n","        if len(frames) < self.max_frames:\n","            # Pad with black frames\n","            padding = [torch.zeros_like(frames[0]) for _ in range(self.max_frames - len(frames))]\n","            frames.extend(padding)\n","        else:\n","            # Truncate to max_frames\n","            frames = frames[:self.max_frames]\n","\n","        frames_tensor = torch.stack(frames)\n","\n","        # Create target mask (1 for selected frames, 0 for others)\n","        target_mask = torch.zeros(self.max_frames)\n","        target_indices = [min(idx, self.max_frames - 1) for idx in self.target_frames[idx]]  # Ensure indices are within bounds\n","        target_mask[target_indices] = 1\n","\n","        return {\n","            'frames': frames_tensor,\n","            'summary': self.summaries[idx],\n","            'target_mask': target_mask\n","        }\n","\n","def custom_collate_fn(batch):\n","    \"\"\"\n","    Custom collate function to handle variable-length frame sequences.\n","    \"\"\"\n","    frames = torch.stack([item['frames'] for item in batch])\n","    summaries = [item['summary'] for item in batch]\n","    target_masks = torch.stack([item['target_mask'] for item in batch])\n","\n","    return {\n","        'frames': frames,\n","        'summary': summaries,\n","        'target_mask': target_masks\n","    }\n","\n","class MultiFrameTrainer:\n","    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n","        self.device = device\n","        self.model = MultiFrameScorer().to(device)\n","\n","    def train(self, train_dataset: MultiFrameDataset,\n","              val_dataset: MultiFrameDataset = None,\n","              batch_size: int = 8, epochs: int = 10):\n","        \"\"\"Train the multi-frame scorer using BCE and cosine similarity loss\"\"\"\n","        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n","        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)  # Learning rate scheduler\n","        criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n","\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=batch_size,\n","            shuffle=True,\n","            num_workers=4,\n","            collate_fn=custom_collate_fn\n","        )\n","\n","        for epoch in range(epochs):\n","            self.model.train()\n","            total_loss = 0\n","\n","            for batch in train_loader:\n","                frames = batch['frames'].to(self.device)\n","                summaries = batch['summary']\n","                target_masks = batch['target_mask'].to(self.device)\n","\n","                optimizer.zero_grad()\n","\n","                # Get model predictions\n","                similarity_scores, _ = self.model(frames, summaries)\n","\n","                # Compute BCE loss\n","                loss = criterion(similarity_scores, target_masks.float())\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  # Gradient clipping\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","\n","            scheduler.step()  # Update learning rate\n","            avg_loss = total_loss / len(train_loader)\n","            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","            # Evaluate on validation set\n","            if val_dataset:\n","                metrics = evaluate_model(self, val_dataset, threshold=0.5)\n","                print(f\"Validation Metrics: {metrics}\")\n","    def select_frames(self, frames: List[str], summary: str, threshold: float = 0.5, max_frames: int = 10) -> List[Dict]:\n","            \"\"\"\n","            Select frames that best match the given summary based on similarity scores.\n","            frames: List of frame paths.\n","            summary: Text summary.\n","            threshold: Similarity score threshold for selecting frames.\n","            max_frames: Maximum number of frames to consider.\n","            Returns: List of selected frames with their scores and positions.\n","            \"\"\"\n","            self.model.eval()\n","\n","            # Prepare images\n","            processed_frames = []\n","            for frame_path in frames:\n","                image = Image.open(frame_path).convert('RGB')\n","                image = self.model.image_transform(image)\n","                processed_frames.append(image)\n","\n","            frames_tensor = torch.stack(processed_frames).unsqueeze(0).to(self.device)\n","\n","            with torch.no_grad():\n","                similarity_scores, _ = self.model(frames_tensor, [summary])\n","                similarity_scores = similarity_scores.squeeze().cpu().numpy()\n","\n","            # Select frames with similarity scores above the threshold\n","            selected_indices = np.where(similarity_scores > threshold)[0]\n","\n","            # Ensure selected indices are within bounds and valid\n","            selected_indices = [idx for idx in selected_indices if idx < max_frames]\n","\n","            # If no frames are selected, return an empty list\n","            if not selected_indices:\n","                return []\n","\n","            # Convert selected_indices to a NumPy array of integers\n","            selected_indices = np.array(selected_indices, dtype=int)\n","\n","            # Sort selected frames by their scores\n","            sorted_indices = selected_indices[np.argsort(-similarity_scores[selected_indices])]\n","\n","            results = []\n","            for idx in sorted_indices:\n","                results.append({\n","                    'frame_path': frames[idx],\n","                    'similarity_score': float(similarity_scores[idx]),\n","                    'position': idx\n","                })\n","\n","            return results\n","\n","def evaluate_model(trainer, dataset, threshold: float = 0.5):\n","    \"\"\"\n","    Evaluate the model on the given dataset.\n","    \"\"\"\n","    trainer.model.eval()\n","    all_targets = []\n","    all_predictions = []\n","    all_frames = []\n","    all_selected_frames = []\n","\n","    for idx in range(len(dataset)):\n","        # Get data\n","        sample = dataset[idx]\n","        frames = sample['frames'].unsqueeze(0).to(trainer.device)\n","        summary = sample['summary']\n","        target_mask = sample['target_mask'].numpy()\n","\n","        # Select frames based on similarity scores\n","        selected_frames = trainer.select_frames(\n","            dataset.frame_sequences[idx],\n","            summary,\n","            threshold=threshold,\n","            max_frames=dataset.max_frames  # Pass max_frames to select_frames\n","        )\n","\n","        # If no frames are selected, skip this example\n","        if not selected_frames:\n","            continue\n","\n","        selected_indices = [frame['position'] for frame in selected_frames]\n","\n","        # Create prediction mask\n","        prediction_mask = torch.zeros_like(sample['target_mask'])\n","        prediction_mask[selected_indices] = 1\n","\n","        # Store results\n","        all_targets.append(target_mask)\n","        all_predictions.append(prediction_mask.numpy())\n","        all_frames.append(frames.squeeze(0).cpu().numpy())\n","        all_selected_frames.append(frames.squeeze(0)[selected_indices].cpu().numpy())\n","\n","    # If no frames were selected for any example, return default metrics\n","    if not all_targets:\n","        return {\n","            'RMSE': float('inf'),\n","            'F1 Score': 0.0,\n","            'SSIM': 0.0,\n","            'PSNR': 0.0\n","        }\n","\n","    # Flatten lists\n","    all_targets = np.concatenate(all_targets)\n","    all_predictions = np.concatenate(all_predictions)\n","    all_frames = np.concatenate(all_frames)\n","    all_selected_frames = np.concatenate(all_selected_frames)\n","\n","    # Compute RMSE\n","    rmse = np.sqrt(np.mean((all_targets - all_predictions) ** 2))\n","\n","    # Compute F1 Score\n","    f1 = f1_score(all_targets, all_predictions, average='binary')\n","\n","    # Compute SSIM and PSNR\n","    ssim_scores = []\n","    psnr_scores = []\n","    for target_frame, selected_frame in zip(all_frames, all_selected_frames):\n","        # Ensure the image is large enough for SSIM\n","        min_side = min(target_frame.shape[0], target_frame.shape[1])\n","        win_size = min(7, min_side)  # Set win_size to the smallest odd value <= min_side\n","        if win_size < 3:  # SSIM requires win_size >= 3\n","            ssim_scores.append(0.0)  # Default value for small images\n","        else:\n","            ssim_scores.append(ssim(\n","                target_frame,\n","                selected_frame,\n","                win_size=win_size,\n","                channel_axis=-1 if target_frame.shape[-1] == 3 else None,\n","                data_range=1.0  # Assuming normalized images (pixel range [0, 1])\n","            ))\n","\n","        # Compute PSNR\n","        psnr_scores.append(psnr(\n","            target_frame,\n","            selected_frame,\n","            data_range=1.0  # Assuming normalized images (pixel range [0, 1])\n","        ))\n","\n","    mean_ssim = np.mean(ssim_scores)\n","    mean_psnr = np.mean(psnr_scores)\n","\n","    return {\n","        'RMSE': rmse,\n","        'F1 Score': f1,\n","        'SSIM': mean_ssim,\n","        'PSNR': mean_psnr\n","    }\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","\n","    # Create training dataset\n","    train_dataset = MultiFrameDataset(\n","        frame_sequences=frame_sequences,\n","        summaries=summaries,\n","        target_frames=target_frames,\n","        transform=MultiFrameScorer().image_transform,\n","        max_frames=20  # Set a maximum number of frames\n","    )\n","\n","    # Create validation dataset (similar to train_dataset)\n","    val_dataset = MultiFrameDataset(\n","        frame_sequences=frame_sequences,\n","        summaries=summaries,\n","        target_frames=target_frames,\n","        transform=MultiFrameScorer().image_transform,\n","        max_frames=20\n","    )\n","    test_dataset = MultiFrameDataset(\n","        frame_sequences=test_frame_seq,\n","        summaries=test_summary,\n","        target_frames=test_target_frames,\n","        transform=MultiFrameScorer().image_transform,\n","        max_frames=20  # Set a maximum number of frames\n","    )\n","    # Train model\n","    trainer = MultiFrameTrainer()\n","    trainer.train(train_dataset, test_dataset, epochs=5)\n","\n","\n","\n",""]},{"cell_type":"code","source":["# Create test dataset\n","    test_dataset = MultiFrameDataset(\n","        frame_sequences=test_frame_seq,\n","        summaries=test_summary,\n","        target_frames=test_target_frames,\n","        transform=MultiFrameScorer().image_transform,\n","        max_frames=20  # Set a maximum number of frames\n","    )\n","\n","    # Evaluate on test dataset\n","    test_metrics = evaluate_model(trainer, test_dataset, threshold=0.5)\n","    print(\"\\nTest Metrics:\")\n","    print(f\"RMSE: {test_metrics['RMSE']:.4f}\")\n","    print(f\"F1 Score: {test_metrics['F1 Score']:.4f}\")\n","    print(f\"SSIM: {test_metrics['SSIM']:.4f}\")\n","    print(f\"PSNR: {test_metrics['PSNR']:.4f}\")"],"metadata":{"id":"83QYZuUuxMxk"},"execution_count":null,"outputs":[]}]}