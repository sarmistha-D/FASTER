{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3aa1ed-479c-4703-8bfb-b30e362e6484",
      "metadata": {
        "id": "ad3aa1ed-479c-4703-8bfb-b30e362e6484"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='7'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8678c0-947a-4a7d-8e36-f59eb149ea27",
      "metadata": {
        "id": "af8678c0-947a-4a7d-8e36-f59eb149ea27"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login('Your Token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575551d2-442b-457e-ab51-53fe65c41204",
      "metadata": {
        "id": "575551d2-442b-457e-ab51-53fe65c41204"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02c19d9-0907-485c-ab82-2956c81a6d29",
      "metadata": {
        "id": "d02c19d9-0907-485c-ab82-2956c81a6d29"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"google/long-t5-tglobal-base\"#\"google/long-t5-tglobal-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac1a213-f68a-44ce-8f68-946d174d7712",
      "metadata": {
        "id": "8ac1a213-f68a-44ce-8f68-946d174d7712"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load CSV files into pandas DataFrames\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "eval_df = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "# Filter DataFrames to retain only the required columns\n",
        "train_filtered = train_df[[\"transcripts\", \"human_summaries\"]]\n",
        "test_filtered = test_df[[\"transcripts\", \"human_summaries\"]]\n",
        "eval_filtered = eval_df[[\"transcripts\", \"human_summaries\"]]\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_filtered)\n",
        "test_dataset = Dataset.from_pandas(test_filtered)\n",
        "eval_dataset = Dataset.from_pandas(eval_filtered)\n",
        "\n",
        "# # Verify datasets\n",
        "# print(\"\\nTrain Dataset Sample:\")\n",
        "# print(train_dataset[0])  # Print the first example from the train dataset\n",
        "\n",
        "# print(\"\\nTest Dataset Sample:\")\n",
        "# print(test_dataset[0])   # Print the first example from the test dataset\n",
        "\n",
        "# print(\"\\nEval Dataset Sample:\")\n",
        "# print(eval_dataset[0])   # Print the first example from the eval dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Summarize the given text: \"\n",
        "def preprocess_function(examples):\n",
        "    #inputs = [prefix + doc for doc in examples[\"transcripts\"]]\n",
        "    inputs = [doc.split(\"\"\"### Response:\\nSummary:\\n```\"\"\")[0] for doc in examples[\"transcripts\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"human_summaries\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "tokenized_train_data = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_data = test_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_eval_data =  eval_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "HFcxfxj0sS2d"
      },
      "id": "HFcxfxj0sS2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df384d60-ce0a-4ce1-be60-0dae3bb03a16",
      "metadata": {
        "id": "df384d60-ce0a-4ce1-be60-0dae3bb03a16"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9414a93d-da19-4a81-bc1a-7bd6a1353f1a",
      "metadata": {
        "id": "9414a93d-da19-4a81-bc1a-7bd6a1353f1a"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38391b6f-43c4-4e3e-9a6b-66b7b3ecdcb5",
      "metadata": {
        "id": "38391b6f-43c4-4e3e-9a6b-66b7b3ecdcb5"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0b6f67-cca4-4c60-940b-1136d90d520e",
      "metadata": {
        "id": "fc0b6f67-cca4-4c60-940b-1136d90d520e"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)#, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5abc7ee-8ff4-40ee-9a19-c9f8450b2e2c",
      "metadata": {
        "id": "a5abc7ee-8ff4-40ee-9a19-c9f8450b2e2c"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec5137f-31a3-49d9-8228-9e1f1ecd607f",
      "metadata": {
        "id": "cec5137f-31a3-49d9-8228-9e1f1ecd607f"
      },
      "outputs": [],
      "source": [
        "model.generation_config.max_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"long_t5_BOS\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    #lr_scheduler_type=\"cosine\",\n",
        "    #load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    #metric_for_best_model=\"eval_loss\",  # Monitor eval_loss (can be changed)\n",
        "    #greater_is_better=False,  # Lower eval_loss is better\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_data,\n",
        "    eval_dataset=tokenized_eval_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # Add EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ccO1qfUUslvL"
      },
      "id": "ccO1qfUUslvL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62c0f10-013c-4b6c-890d-470e70947180",
      "metadata": {
        "id": "f62c0f10-013c-4b6c-890d-470e70947180"
      },
      "outputs": [],
      "source": [
        "# trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de470fe7-7d23-4663-b6c7-4c295a12c82e",
      "metadata": {
        "id": "de470fe7-7d23-4663-b6c7-4c295a12c82e"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./long_sum\")\n",
        "tokenizer.save_pretrained(\"./long_sum\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Evaluation Measurement\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer  # Use AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import csv\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "eval_df =  pd.read_csv(\"eval.csv\")\n",
        "\n",
        "# Filter DataFrames to retain only the required columns\n",
        "train_filtered = train_df[[\"transcripts\", \"human_summaries\"]]\n",
        "test_filtered = test_df[[\"video_ids\", \"transcripts\", \"human_summaries\"]]\n",
        "eval_filtered = test_df[[\"video_ids\",\"transcripts\", \"human_summaries\"]]\n",
        "\n",
        "# Convert the filtered data into Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_filtered)\n",
        "test_dataset = Dataset.from_pandas(test_filtered)\n",
        "eval_dataset = Dataset.from_pandas(eval_filtered)\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./long_sum\").to(device)  # Use AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./long_sum\")  # Use AutoTokenizer\n",
        "\n",
        "# Function to generate summaries\n",
        "def generate_summary(model, tokenizer, dataset, max_length=128):\n",
        "    model.eval()\n",
        "    summaries = []\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        inputs = tokenizer(\n",
        "            example[\"transcripts\"].split(\"\"\"### Response:\\nSummary:\\n```\"\"\")[0],  # Add task prefix\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=1024,  # Adjust based on your model's max input length\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        ).to(device)  # Move inputs to GPU\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Generate summaries for the evaluation dataset\n",
        "predicted_summaries = generate_summary(model, tokenizer, eval_dataset)\n",
        "\n",
        "# Ground truths\n",
        "ground_truths = eval_df[\"human_summaries\"].tolist()\n",
        "\n",
        "# Load metrics\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bert_score_metric = evaluate.load(\"bertscore\")\n",
        "\n",
        "# BLEU calculation function\n",
        "def calculate_bleu(pred, ref):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    return [\n",
        "        sentence_bleu([ref], pred, weights=(1, 0, 0, 0), smoothing_function=smoothing),\n",
        "        sentence_bleu([ref], pred, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing),\n",
        "        sentence_bleu([ref], pred, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
        "    ]\n",
        "\n",
        "# Compute metrics\n",
        "rouge_scores = rouge_metric.compute(predictions=predicted_summaries, references=ground_truths)\n",
        "bleu_scores = [calculate_bleu(pred, ref) for pred, ref in zip(predicted_summaries, ground_truths)]\n",
        "bert_scores = bert_score_metric.compute(predictions=predicted_summaries, references=ground_truths, lang=\"en\")\n",
        "\n",
        "# Summarize metrics\n",
        "metrics_summary = {\n",
        "    \"ROUGE-1\": np.mean(rouge_scores[\"rouge1\"]),\n",
        "    \"ROUGE-2\": np.mean(rouge_scores[\"rouge2\"]),\n",
        "    \"ROUGE-L\": np.mean(rouge_scores[\"rougeL\"]),\n",
        "    \"BLEU-1\": np.mean([score[0] for score in bleu_scores]),\n",
        "    \"BLEU-2\": np.mean([score[1] for score in bleu_scores]),\n",
        "    \"BLEU-3\": np.mean([score[2] for score in bleu_scores]),\n",
        "    \"BERT Score\": np.mean(bert_scores['f1'])\n",
        "}\n",
        "\n",
        "# Print overall average scores\n",
        "print(\"\\nOverall Metrics Summary:\")\n",
        "for metric, score in metrics_summary.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "# Save metrics summary\n",
        "summary_csv = \"eval/long_sum.csv\"\n",
        "with open(summary_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Metric\", \"Score\"])\n",
        "    for metric, score in metrics_summary.items():\n",
        "        writer.writerow([metric, score])\n",
        "print(f\"Metrics summary saved to {summary_csv}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h-Rt3L7bsyn7"
      },
      "id": "h-Rt3L7bsyn7",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VjwWPop8s-kI"
      },
      "id": "VjwWPop8s-kI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (vinayak)",
      "language": "python",
      "name": "vinayak"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}