{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ed7a4d-a871-4153-914c-eaf784d3578d",
      "metadata": {
        "id": "e3ed7a4d-a871-4153-914c-eaf784d3578d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import DPOTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c29d82-e29d-46d4-ba44-5a46246dc110",
      "metadata": {
        "id": "c4c29d82-e29d-46d4-ba44-5a46246dc110",
        "outputId": "7867ca8d-0052-4272-a399-6846fc0c5359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
            "A possible explanation is you have a new CUDA version which isn't\n",
            "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
            "We shall now use Xformers instead, which does not have any performance hits!\n",
            "We found this negligible impact by benchmarking on 1x A100.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
            "To install flash-attn, do the below:\n",
            "\n",
            "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
            "==((====))==  Unsloth 2024.12.12: Fast Gemma2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB MIG 7g.80gb. Max memory: 79.151 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.12.12 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import  AutoTokenizer ,TrainingArguments\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "\n",
        "if True:\n",
        "    model, _ = FastLanguageModel.from_pretrained(\n",
        "        model_name = 'gemma_unsloth_dpo',#\"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/gemma-tokenizer-chatml\", use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064d06e7-9b37-4374-a2a4-f27e24644e30",
      "metadata": {
        "scrolled": true,
        "id": "064d06e7-9b37-4374-a2a4-f27e24644e30"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# sample dataset creation\n",
        "dataset = {}\n",
        "with open('curr_dpo_dataset_train.json','r') as f:\n",
        "    train = json.load(f)\n",
        "with open('curr_dpo_dataset_eval.json','r') as f:\n",
        "    test = json.load(f)\n",
        "\n",
        "\n",
        "train_dict = {'chosen':[],\n",
        "              'rejected':[]}\n",
        "\n",
        "eval_dict = {'chosen':[],\n",
        "              'rejected':[]}\n",
        "\n",
        "for k in range(len(train['ids'])):\n",
        "    data_chosen = [{\n",
        "        'content':train['prompt'][k],\n",
        "        'role':'user',\n",
        "    },\n",
        "          {  'content':train['rank 1'][k],\n",
        "        'role':'assistant',\n",
        "    }]\n",
        "\n",
        "    data_rejected = [{\n",
        "        'content':train['prompt'][k],\n",
        "        'role':'user',\n",
        "    },\n",
        "            {'content':train['rank 2'][k],\n",
        "        'role':'assistant',\n",
        "    }]\n",
        "    train_dict['chosen'].append(data_chosen)\n",
        "    train_dict['rejected'].append(data_rejected)\n",
        "for k in range(len(test['ids'])):\n",
        "    data_chosen = [{\n",
        "        'content':test['prompt'][k],\n",
        "        'role':'user',\n",
        "    },\n",
        "          {  'content':test['rank 1'][k],\n",
        "        'role':'assistant',\n",
        "    }]\n",
        "\n",
        "    data_rejected = [{\n",
        "        'content':test['prompt'][k],\n",
        "        'role':'user',\n",
        "    },\n",
        "            {'content':test['rank 2'][k],\n",
        "        'role':'assistant',\n",
        "    }]\n",
        "    eval_dict['chosen'].append(data_chosen)\n",
        "    eval_dict['rejected'].append(data_rejected)\n",
        "\n",
        "\n",
        "from datasets import Dataset,  DatasetDict\n",
        "dataset_train = Dataset.from_dict(train_dict)\n",
        "dataset_test = Dataset.from_dict(eval_dict)\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "hLcxuw-qnFty"
      },
      "id": "hLcxuw-qnFty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e1b231-c3e0-4bd6-b39a-1562c3c85c8b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c813e70749d749579efc9ff2fdd58800",
            "17ddf1c179a34182ad9526c3d5b2b659"
          ]
        },
        "id": "e9e1b231-c3e0-4bd6-b39a-1562c3c85c8b",
        "outputId": "38a0ed4f-3136-4306-f4d9-846340d73d8e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c813e70749d749579efc9ff2fdd58800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Formatting comparisons with prompt template:   0%|          | 0/283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17ddf1c179a34182ad9526c3d5b2b659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Formatting comparisons with prompt template:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "column_names = list(dataset[\"train\"].features)\n",
        "def apply_dpo_template(example):\n",
        "  if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "    # For DPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue\n",
        "    # We therefore need to extract the N-1 turns to form the prompt\n",
        "    prompt_messages = example[\"chosen\"][:-1]\n",
        "\n",
        "\n",
        "    # Now we extract the final turn to define chosen/rejected responses\n",
        "    chosen_messages = example[\"chosen\"][-1:]\n",
        "    rejected_messages = example[\"rejected\"][-1:]\n",
        "    example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "    example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "    example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
        "  return example\n",
        "\n",
        "dataset = dataset.map(apply_dpo_template,remove_columns=column_names,\n",
        "          desc=\"Formatting comparisons with prompt template\",)\n",
        "for split in [\"train\", \"test\"]:\n",
        "    dataset[split] = dataset[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['chosen'][0],dataset['train']['rejected'][0],dataset['train']['prompt'][0]"
      ],
      "metadata": {
        "id": "KKNP4CcdnIiJ"
      },
      "id": "KKNP4CcdnIiJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723473b1-ef96-44d0-a8f6-4be67441710e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a5f166730ec4aadbc3bf44759cd6282",
            "9b23a26cf6c44a40b1b4d9d73a13d20b",
            "6aa091f5aeb846dcafd3727777a2b28e",
            "9eca83eca5bd44e2bcc5b1ff9585b10a",
            "4079ac284b724e7fa6f6f03081e87cb5",
            "da571dc748de497aa908b5e6642f6724"
          ]
        },
        "id": "723473b1-ef96-44d0-a8f6-4be67441710e",
        "outputId": "ee6836bb-ec66-49a1-e79d-9445af87c2b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sarmistha/miniconda3/envs/smol/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/sarmistha/miniconda3/envs/smol/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a5f166730ec4aadbc3bf44759cd6282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt from train dataset:   0%|          | 0/283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b23a26cf6c44a40b1b4d9d73a13d20b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aa091f5aeb846dcafd3727777a2b28e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt from eval dataset:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eca83eca5bd44e2bcc5b1ff9585b10a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to eval dataset:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4079ac284b724e7fa6f6f03081e87cb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da571dc748de497aa908b5e6642f6724",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "        do_eval=True,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        #eval_steps = 100,\n",
        "        #save_strategy = \"epoch\",\n",
        "        per_device_train_batch_size = 1, #Zephyr\n",
        "        gradient_accumulation_steps = 16, #Zephyr\n",
        "        per_device_eval_batch_size = 2,\n",
        "        warmup_ratio = 0.1, #Zephyr\n",
        "        num_train_epochs = 5, #Zephyr\n",
        "        learning_rate = 5.0e-07, #Zephyr\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"paged_adamw_8bit\",\n",
        "        lr_scheduler_type = \"cosine\", #Zephyr\n",
        "        seed = 3407,\n",
        "        output_dir = \"./gemma9b_DPO_curr/\",\n",
        ")\n",
        "\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.05, #Zephyr\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "uHIATx1rmjrr"
      },
      "id": "uHIATx1rmjrr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccedfd5-a45c-487f-b71c-581200c611d3",
      "metadata": {
        "id": "0ccedfd5-a45c-487f-b71c-581200c611d3"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"lora_model\") # Local saving\n",
        "# tokenizer.save_pretrained(\"lora_model\")\n",
        "from huggingface_hub import login\n",
        "\n",
        "login('Your ID')\n",
        "\n",
        "model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python SMOL",
      "language": "python",
      "name": "smol"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}