# FASTER

# Overview
In the current era of digital financial collaboration, financial advisory videos have emerged as a critical medium of engagement, particularly among youth, retail investors, and individuals seeking structured guidance for financial planning. These videos, predominantly disseminated via multimodal platforms such as YouTube podcasts and advisory videos, often span 30â€“40 minutes, posing significant cognitive and temporal challenges for users aiming to distill actionable insights within limited time frames. Consequently, there is an urgent need for an intelligent framework capable of generating concise summaries of such lengthy content while aligning relevant visual keyframes with the extracted textual summaries. Although contemporary Vision-Language Models (VLMs) have demonstrated potential in this domain, their efficacy is limited due to the lack of domain-specific pretraining tailored to the financial advisory context. Two critical research gaps remain unaddressed: (1) financial advisory videos typically exhibit minimal visual dynamism, making the retrieval of informative visual cues and speaker attribution particularly challenging during summary generation; and (2) the absence of a dedicated, domain-specific multimodal dataset further hampers the development and benchmarking of effective summarization systems in this space. In this paper, we introduce FASTER (Financial Advisory Summariser with Textually Embedded Relevant images), a modular and domain-adaptive framework engineered to effectively summarise long-form multimodal financial advisory videos, typically ranging from 30 to 40 minutes. FASTER is designed to address three fundamental challenges in this domain: (i) extraction of modality-specific features, (ii) generation of coherent and optimised textual summaries, and (iii) alignment of visual keyframes with semantically relevant textual content. To this end, the framework leverages BLIP-2 for high-level semantic visual grounding, OCR for text extraction from visual frames, and Whisper with Speaker diarization to construct a composite BOS (BLIP-OCR-Speaker Information) feature set. A customised modified Direct Preference Optimisation (DPO)-based training objective, augmented with BOS-guided factual verification, enforces precision, domain relevance, and consistency with human-authored references. Furthermore, FASTER incorporates a ranker-based keyframe retrieval module that enhances interpretability by tightly coupling visual segments with their corresponding summarised text. To facilitate reproducibility and catalyse further research in financial multimodal understanding, we release Fin-APT, a curated dataset comprising 470 publicly available financial pep-talk videos. Our comprehensive cross-domain evaluations demonstrate that FASTER consistently outperforms state-of-the-art LLMs and VLMs in terms of robustness, generalisation, and summarisation quality, thus establishing a strong new benchmark for financial multimodal summarisation and improving accessibility to long-form advisory content.
